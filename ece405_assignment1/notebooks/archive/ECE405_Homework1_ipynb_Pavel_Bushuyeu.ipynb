{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ijR6qPINU_P6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLM-from-scratch'...\n",
      "remote: Enumerating objects: 539, done.\u001b[K\n",
      "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
      "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
      "remote: Total 539 (delta 118), reused 135 (delta 107), pack-reused 384 (from 1)\u001b[K\n",
      "Receiving objects: 100% (539/539), 51.92 MiB | 41.90 MiB/s, done.\n",
      "Resolving deltas: 100% (281/281), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/bushuyeu/LLM-from-scratch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S5eKjGhFMiZ6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/LLM-from-scratch/assignment1\n"
     ]
    }
   ],
   "source": [
    "%cd LLM-from-scratch/assignment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uVh85UKdMng8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 8, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 5 (delta 1), reused 5 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (5/5), 3.51 KiB | 1.17 MiB/s, done.\n",
      "From https://github.com/bushuyeu/LLM-from-scratch\n",
      "   f4ef073..0368f4a  main       -> origin/main\n",
      "Updating f4ef073..0368f4a\n",
      "Fast-forward\n",
      " assignment1/ece496b_basics/tokenizer.py | 239 \u001b[32m++++++++++++++++++++++++++++++++\u001b[m\n",
      " 1 file changed, 239 insertions(+)\n",
      " create mode 100644 assignment1/ece496b_basics/tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MLdwVyZ3MKk8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already on 'main'\n",
      "Your branch is up to date with 'origin/main'.\n"
     ]
    }
   ],
   "source": [
    "!git checkout main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUpCszLnfDf1"
   },
   "source": [
    "No need for Conda environment on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "D1tPK7iKenBW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/LLM-from-scratch/assignment1\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: einops>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (0.8.2)\n",
      "Requirement already satisfied: einx>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (0.3.0)\n",
      "Requirement already satisfied: jaxtyping>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (0.3.9)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (2.0.2)\n",
      "Requirement already satisfied: psutil>=6.1.1 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (7.2.2)\n",
      "Requirement already satisfied: pytest>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (8.4.2)\n",
      "Requirement already satisfied: regex>=2024.11.6 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (2025.11.3)\n",
      "Requirement already satisfied: submitit>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (1.5.4)\n",
      "Requirement already satisfied: tiktoken>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (0.12.0)\n",
      "Requirement already satisfied: torch~=2.6.0 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (2.6.0)\n",
      "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (4.67.3)\n",
      "Requirement already satisfied: wandb>=0.19.7 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (0.25.0)\n",
      "Requirement already satisfied: ty>=0.0.1a16 in /usr/local/lib/python3.12/dist-packages (from cs336-basics==1.0.6) (0.0.18)\n",
      "\u001b[33mWARNING: cs336-basics 1.0.6 does not provide the extra 'test'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from einx>=0.3.0->cs336-basics==1.0.6) (1.13.1)\n",
      "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from einx>=0.3.0->cs336-basics==1.0.6) (2.4.7)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping>=0.3.0->cs336-basics==1.0.6) (0.1.7)\n",
      "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.4->cs336-basics==1.0.6) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.4->cs336-basics==1.0.6) (26.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.4->cs336-basics==1.0.6) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.4->cs336-basics==1.0.6) (2.19.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from submitit>=1.5.2->cs336-basics==1.0.6) (3.1.2)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.12/dist-packages (from submitit>=1.5.2->cs336-basics==1.0.6) (4.15.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.9.0->cs336-basics==1.0.6) (2.32.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (3.24.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->cs336-basics==1.0.6) (75.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->einx>=0.3.0->cs336-basics==1.0.6) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->cs336-basics==1.0.6) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->cs336-basics==1.0.6) (3.1.46)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->cs336-basics==1.0.6) (4.9.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->cs336-basics==1.0.6) (5.29.6)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->cs336-basics==1.0.6) (2.12.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->cs336-basics==1.0.6) (6.0.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->cs336-basics==1.0.6) (2.53.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.19.7->cs336-basics==1.0.6) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.19.7->cs336-basics==1.0.6) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.19.7->cs336-basics==1.0.6) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.19.7->cs336-basics==1.0.6) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.9.0->cs336-basics==1.0.6) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.9.0->cs336-basics==1.0.6) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.9.0->cs336-basics==1.0.6) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.9.0->cs336-basics==1.0.6) (2026.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch~=2.6.0->cs336-basics==1.0.6) (3.0.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.19.7->cs336-basics==1.0.6) (5.0.2)\n",
      "Building wheels for collected packages: cs336-basics\n",
      "  Building editable for cs336-basics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for cs336-basics: filename=cs336_basics-1.0.6-py3-none-any.whl size=3237 sha256=6c6cdd49e59e2396ccb791f758e7b27459316d1c43c8b91ea233e53b965ea6b3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ij6whtyq/wheels/11/33/89/135a2c84cfc3609d8bf16452627bea272b9f0c2a923d4c36c9\n",
      "Successfully built cs336-basics\n",
      "Installing collected packages: cs336-basics\n",
      "  Attempting uninstall: cs336-basics\n",
      "    Found existing installation: cs336-basics 1.0.6\n",
      "    Uninstalling cs336-basics-1.0.6:\n",
      "      Successfully uninstalled cs336-basics-1.0.6\n",
      "Successfully installed cs336-basics-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -e .'[test]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J-_dABLeW3LR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n",
      "rootdir: /content/LLM-from-scratch/assignment1\n",
      "configfile: pytest.ini\n",
      "plugins: jaxtyping-0.3.9, anyio-4.12.1, typeguard-4.5.0, langsmith-0.7.3\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_train_bpe.py::test_train_bpe_speed \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 33%]\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 66%]\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe_special_tokens \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.90s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/test_train_bpe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "5wrA646Gf3Ht"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "/content/data\n",
      "TinyStoriesV2-GPT4- 100%[===================>]   2.07G   350MB/s    in 7.0s    \n",
      "TinyStoriesV2-GPT4- 100%[===================>]  21.46M  48.8MB/s    in 0.4s    \n",
      "owt_train.txt.gz    100%[===================>]   4.28G  93.7MB/s    in 45s     \n",
      "owt_valid.txt.gz    100%[===================>] 106.61M  64.6MB/s    in 1.7s    \n",
      "\n",
      "=== Download verification ===\n",
      "TinyStoriesV2-GPT4-train.txt: 2.23 GB\n",
      "TinyStoriesV2-GPT4-valid.txt: 0.02 GB\n",
      "owt_train.txt: 11.92 GB\n",
      "owt_valid.txt: 0.29 GB\n",
      "/content/LLM-from-scratch/assignment1\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "\n",
    "# Remove old data folder and start fresh\n",
    "!rm -rf data\n",
    "!mkdir -p data\n",
    "%cd data\n",
    "\n",
    "# Download TinyStories\n",
    "!wget -q --show-progress https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\n",
    "!wget -q --show-progress https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt\n",
    "\n",
    "# Download OpenWebText sample\n",
    "!wget -q --show-progress https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz\n",
    "!gunzip owt_train.txt.gz\n",
    "!wget -q --show-progress https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz\n",
    "!gunzip owt_valid.txt.gz\n",
    "\n",
    "# Verify all downloads\n",
    "import os\n",
    "print(\"\\n=== Download verification ===\")\n",
    "for f in [\n",
    "    \"TinyStoriesV2-GPT4-train.txt\",\n",
    "    \"TinyStoriesV2-GPT4-valid.txt\",\n",
    "    \"owt_train.txt\",\n",
    "    \"owt_valid.txt\"\n",
    "    ]:\n",
    "    if os.path.exists(f):\n",
    "        size = os.path.getsize(f)\n",
    "        print(f\"{f}: {size / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(f\"{f}: MISSING!\")\n",
    "\n",
    "%cd /content/LLM-from-scratch/assignment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "imports_and_helpers"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and helpers loaded.\n"
     ]
    }
   ],
   "source": [
    "# Imports and helper functions\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import tracemalloc\n",
    "from ece496b_basics import train_bpe\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "def safe_decode(b):\n",
    "    \"\"\"Safely decode bytes to string, falling back to repr if UTF-8 fails.\"\"\"\n",
    "    try:\n",
    "        return b.decode('utf-8')\n",
    "    except:\n",
    "        return repr(b)\n",
    "\n",
    "def analyze_vocab(vocab, name):\n",
    "    \"\"\"Print analysis of a vocabulary.\"\"\"\n",
    "    longest_token = max(vocab.values(), key=len)\n",
    "    avg_len = sum(len(v) for v in vocab.values()) / len(vocab)\n",
    "    \n",
    "    print(f\"\\n{name} Vocabulary Analysis:\")\n",
    "    print(f\"  Total tokens: {len(vocab)}\")\n",
    "    print(f\"  Merged tokens (non-byte): {len([k for k in vocab if k >= 256])}\")\n",
    "    print(f\"  Average token length: {avg_len:.2f} bytes\")\n",
    "    print(f\"  Longest token: '{safe_decode(longest_token)}' ({len(longest_token)} bytes)\")\n",
    "    \n",
    "    # Top 5 longest\n",
    "    print(f\"  Top 5 longest tokens:\")\n",
    "    for tid, tbytes in sorted(vocab.items(), key=lambda x: len(x[1]), reverse=True)[:5]:\n",
    "        print(f\"    ID {tid}: '{safe_decode(tbytes)}' ({len(tbytes)} bytes)\")\n",
    "\n",
    "print(\"Imports and helpers loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tinystories_header"
   },
   "source": [
    "## TinyStories BPE Training (vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ts_training"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 700.46 seconds (11.67 minutes)\n",
      "Peak memory: 0.08 GB\n",
      "Number of merges: 9743\n",
      "\n",
      "TinyStories Vocabulary Analysis:\n",
      "  Total tokens: 10000\n",
      "  Merged tokens (non-byte): 9744\n",
      "  Average token length: 5.79 bytes\n",
      "  Longest token: ' accomplishment' (15 bytes)\n",
      "  Top 5 longest tokens:\n",
      "    ID 7159: ' accomplishment' (15 bytes)\n",
      "    ID 9142: ' disappointment' (15 bytes)\n",
      "    ID 9378: ' responsibility' (15 bytes)\n",
      "    ID 3227: ' uncomfortable' (14 bytes)\n",
      "    ID 3514: ' compassionate' (14 bytes)\n",
      "\n",
      "Saved to outputs/ts_vocab_10k.pkl and outputs/ts_merges_10k.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train BPE on TinyStories\n",
    "tracemalloc.start()\n",
    "start_time = time.time()\n",
    "\n",
    "ts_vocab, ts_merges = train_bpe(\n",
    "    input_path=\"/content/data/TinyStoriesV2-GPT4-train.txt\",\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"<|endoftext|>\"]\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "_, peak_mem = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"Training time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "print(f\"Peak memory: {peak_mem / 1e9:.2f} GB\")\n",
    "print(f\"Number of merges: {len(ts_merges)}\")\n",
    "\n",
    "analyze_vocab(ts_vocab, \"TinyStories\")\n",
    "\n",
    "# Save immediately\n",
    "with open(\"outputs/ts_vocab_10k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ts_vocab, f)\n",
    "with open(\"outputs/ts_merges_10k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ts_merges, f)\n",
    "print(\"\\nSaved to outputs/ts_vocab_10k.pkl and outputs/ts_merges_10k.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owt_header"
   },
   "source": [
    "## OpenWebText BPE Training (vocab_size=32000)\n",
    "\n",
    "**Problem (train_bpe_expts_owt)**: Train a byte-level BPE tokenizer on OpenWebText with vocab_size=32,000.\n",
    "\n",
    "Resource requirements: ≤12 hours (no GPUs), ≤100GB RAM\n",
    "\n",
    "**Note**: This will take several hours. Consider running overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 3.2%\n",
      "RAM available: 53.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Check current memory usage\n",
    "import psutil\n",
    "print(f\"RAM used: {psutil.virtual_memory().percent}%\")\n",
    "print(f\"RAM available: {psutil.virtual_memory().available / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "owt_training"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OWT pkl files already present in outputs/\n"
     ]
    }
   ],
   "source": [
    "# # Train BPE on OpenWebText with vocab_size=32000\n",
    "# # Takes several hours — skip if you already have the pkl files.\n",
    "# # Upload owt_vocab_32k.pkl and owt_merges_32k.pkl to outputs/ instead.\n",
    "#\n",
    "# tracemalloc.start()\n",
    "# start_time = time.time()\n",
    "#\n",
    "# owt_vocab, owt_merges = train_bpe(\n",
    "#     input_path=\"/content/data/owt_train.txt\",\n",
    "#     vocab_size=32000,\n",
    "#     special_tokens=[\"<|endoftext|>\"]\n",
    "# )\n",
    "#\n",
    "# elapsed_time = time.time() - start_time\n",
    "# _, peak_mem = tracemalloc.get_traced_memory()\n",
    "# tracemalloc.stop()\n",
    "#\n",
    "# print(\"=\"*50)\n",
    "# print(\"OpenWebText BPE Training Complete!\")\n",
    "# print(\"=\"*50)\n",
    "# print(f\"Training time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes, {elapsed_time/3600:.2f} hours)\")\n",
    "# print(f\"Peak memory: {peak_mem / 1e9:.2f} GB\")\n",
    "# print(f\"Number of merges: {len(owt_merges)}\")\n",
    "#\n",
    "# analyze_vocab(owt_vocab, \"OpenWebText\")\n",
    "#\n",
    "# with open(\"outputs/owt_vocab_32k.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(owt_vocab, f)\n",
    "# with open(\"outputs/owt_merges_32k.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(owt_merges, f)\n",
    "# print(\"\\nSaved to outputs/owt_vocab_32k.pkl and outputs/owt_merges_32k.pkl\")\n",
    "\n",
    "# Upload pre-trained OWT vocab/merges instead\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "needed = [f for f in [\"outputs/owt_vocab_32k.pkl\", \"outputs/owt_merges_32k.pkl\"] if not os.path.exists(f)]\n",
    "if needed:\n",
    "    print(\"Upload owt_vocab_32k.pkl and owt_merges_32k.pkl:\")\n",
    "    uploaded = files.upload()\n",
    "    for name, data in uploaded.items():\n",
    "        dest = os.path.join(\"outputs\", name)\n",
    "        with open(dest, \"wb\") as f:\n",
    "            f.write(data)\n",
    "        print(f\"  Saved {dest} ({len(data)} bytes)\")\n",
    "else:\n",
    "    print(\"OWT pkl files already present in outputs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison_header"
   },
   "source": [
    "## Compare TinyStories vs OpenWebText Tokenizers\n",
    "\n",
    "**Problem (train_bpe_expts_owt) Part (b)**: Compare and contrast the tokenizers trained on TinyStories vs OpenWebText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "comparison_load"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyStories vocab already in memory\n",
      "Loaded OpenWebText vocab from disk\n"
     ]
    }
   ],
   "source": [
    "# Load vocabularies (in case running from fresh kernel after long OWT training)\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    ts_vocab\n",
    "    print(\"TinyStories vocab already in memory\")\n",
    "except NameError:\n",
    "    with open(\"outputs/ts_vocab_10k.pkl\", \"rb\") as f:\n",
    "        ts_vocab = pickle.load(f)\n",
    "    print(\"Loaded TinyStories vocab from disk\")\n",
    "\n",
    "try:\n",
    "    owt_vocab\n",
    "    print(\"OpenWebText vocab already in memory\")\n",
    "except NameError:\n",
    "    with open(\"outputs/owt_vocab_32k.pkl\", \"rb\") as f:\n",
    "        owt_vocab = pickle.load(f)\n",
    "    print(\"Loaded OpenWebText vocab from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "comparison_analysis"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyStories merged tokens: 9744\n",
      "OpenWebText merged tokens: 31744\n",
      "\n",
      "Shared tokens: 7063\n",
      "  (72.5% of TinyStories tokens are in OWT)\n",
      "  (22.2% of OWT tokens are in TinyStories)\n",
      "\n",
      "Tokens unique to TinyStories: 2681\n",
      "Tokens unique to OpenWebText: 24681\n"
     ]
    }
   ],
   "source": [
    "# Compare token sets (excluding base 256 bytes)\n",
    "ts_tokens = set(v for k, v in ts_vocab.items() if k >= 256)\n",
    "owt_tokens = set(v for k, v in owt_vocab.items() if k >= 256)\n",
    "\n",
    "shared = ts_tokens & owt_tokens\n",
    "ts_only = ts_tokens - owt_tokens\n",
    "owt_only = owt_tokens - ts_tokens\n",
    "\n",
    "print(f\"TinyStories merged tokens: {len(ts_tokens)}\")\n",
    "print(f\"OpenWebText merged tokens: {len(owt_tokens)}\")\n",
    "print(f\"\\nShared tokens: {len(shared)}\")\n",
    "print(f\"  ({len(shared)/len(ts_tokens)*100:.1f}% of TinyStories tokens are in OWT)\")\n",
    "print(f\"  ({len(shared)/len(owt_tokens)*100:.1f}% of OWT tokens are in TinyStories)\")\n",
    "print(f\"\\nTokens unique to TinyStories: {len(ts_only)}\")\n",
    "print(f\"Tokens unique to OpenWebText: {len(owt_only)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "comparison_examples"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 longest tokens UNIQUE to TinyStories (children's stories):\n",
      "  ' congratulated'\n",
      "  ' granddaughter'\n",
      "  ' marshmallows'\n",
      "  ' veterinarian'\n",
      "  ' strawberries'\n",
      "  ' imaginations'\n",
      "  ' caterpillars'\n",
      "  ' stethoscope'\n",
      "  ' grandparent'\n",
      "  ' firefighter'\n",
      "  ' disagreeing'\n",
      "  ' screwdriver'\n",
      "  ' adventurous'\n",
      "  ' reluctantly'\n",
      "  ' automobiles'\n",
      "\n",
      "Top 15 longest tokens UNIQUE to OpenWebText (web text):\n",
      "  'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ'\n",
      "  '----------------------------------------------------------------'\n",
      "  '————————————————'\n",
      "  '================================'\n",
      "  '________________________________'\n",
      "  '********************************'\n",
      "  'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ'\n",
      "  '................................'\n",
      "  '--------------------------------'\n",
      "  '————————'\n",
      "  ' disproportionately'\n",
      "  ' telecommunications'\n",
      "  ' environmentalists'\n",
      "  ' responsibilities'\n",
      "  ' counterterrorism'\n"
     ]
    }
   ],
   "source": [
    "# Helper function (in case cell-8 wasn't run)\n",
    "def safe_decode(b):\n",
    "    try:\n",
    "        return b.decode('utf-8')\n",
    "    except:\n",
    "        return repr(b)\n",
    "\n",
    "# Show example tokens unique to each dataset (sorted by length for interesting examples)\n",
    "print(\"Top 15 longest tokens UNIQUE to TinyStories (children's stories):\")\n",
    "for t in sorted(ts_only, key=len, reverse=True)[:15]:\n",
    "    print(f\"  '{safe_decode(t)}'\")\n",
    "\n",
    "print(f\"\\nTop 15 longest tokens UNIQUE to OpenWebText (web text):\")\n",
    "for t in sorted(owt_only, key=len, reverse=True)[:15]:\n",
    "    print(f\"  '{safe_decode(t)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "comparison_stats"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Summary Statistics\n",
      "==================================================\n",
      "Metric                          TinyStories  OpenWebText\n",
      "--------------------------------------------------\n",
      "Vocab size                            10000        32000\n",
      "Avg token length (bytes)               5.79         6.34\n",
      "Longest token (bytes)                    15           64\n",
      "--------------------------------------------------\n",
      "TinyStories longest: ' accomplishment'\n",
      "OpenWebText longest: 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ'\n"
     ]
    }
   ],
   "source": [
    "# Compare statistics\n",
    "ts_longest = max(ts_vocab.values(), key=len)\n",
    "owt_longest = max(owt_vocab.values(), key=len)\n",
    "ts_avg_len = sum(len(v) for v in ts_vocab.values()) / len(ts_vocab)\n",
    "owt_avg_len = sum(len(v) for v in owt_vocab.values()) / len(owt_vocab)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Summary Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<30} {'TinyStories':>12} {'OpenWebText':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Vocab size':<30} {len(ts_vocab):>12} {len(owt_vocab):>12}\")\n",
    "print(f\"{'Avg token length (bytes)':<30} {ts_avg_len:>12.2f} {owt_avg_len:>12.2f}\")\n",
    "print(f\"{'Longest token (bytes)':<30} {len(ts_longest):>12} {len(owt_longest):>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"TinyStories longest: '{safe_decode(ts_longest)}'\")\n",
    "print(f\"OpenWebText longest: '{safe_decode(owt_longest)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longest OpenWebText Token (Helper)\n",
    "\n",
    "Run this cell to report the longest token in the trained OpenWebText vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest token ID: 25821\n",
      "Longest token length: 64 bytes\n",
      "Longest token (safe decode): ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ\n",
      "Longest token (raw bytes): b'\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82'\n"
     ]
    }
   ],
   "source": [
    "# Find the longest token in OpenWebText vocabulary\n",
    "import pickle\n",
    "try:\n",
    "    owt_vocab\n",
    "except NameError:\n",
    "    with open(\"outputs/owt_vocab_32k.pkl\", \"rb\") as f:\n",
    "        owt_vocab = pickle.load(f)\n",
    "# Re-define safe_decode if this is a fresh kernel\n",
    "def safe_decode(b):\n",
    "    try:\n",
    "        return b.decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        return repr(b)\n",
    "longest_id, longest_token = max(owt_vocab.items(), key=lambda x: len(x[1]))\n",
    "print(f\"Longest token ID: {longest_id}\")\n",
    "print(f\"Longest token length: {len(longest_token)} bytes\")\n",
    "print(f\"Longest token (safe decode): {safe_decode(longest_token)}\")\n",
    "print(f\"Longest token (raw bytes): {repr(longest_token)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answers_header"
   },
   "source": [
    "## Answers to Assignment Questions\n",
    "\n",
    "### Part (a): What is the longest token in the OWT vocabulary? Does it make sense?\n",
    "\n",
    "*Fill in your answer here after running the cells above.*\n",
    "\n",
    "### Part (b): Compare and contrast TinyStories vs OpenWebText tokenizers.\n",
    "\n",
    "**Important**: Note that this comparison uses different vocab sizes (TinyStories: 10k, OWT: 32k) as specified by the assignment. When writing your answer, acknowledge this limitation and focus on qualitative differences (token types, domain-specific vocabulary) rather than raw token counts.\n",
    "\n",
    "*Fill in your answer here. Consider:*\n",
    "- *The vocab size difference (10k vs 32k) means OWT naturally has more tokens*\n",
    "- *What % of TinyStories tokens also appear in OWT? What does this suggest?*\n",
    "- *Types of tokens unique to each (children's vocabulary vs web/technical terms)*\n",
    "- *Average token length differences and what they indicate about text complexity*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "profiling_header"
   },
   "source": [
    "## Optional: Profiling (TinyStories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "profiling"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         6507812 function calls (6507706 primitive calls) in 259.846 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 326 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      259  255.719    0.987  497.458    1.921 {built-in method time.sleep}\n",
      "      174    0.000    0.000  479.488    2.756 /usr/lib/python3.12/multiprocessing/pool.py:500(_wait_for_updates)\n",
      "        1    0.000    0.000  240.729  240.729 /usr/lib/python3.12/multiprocessing/pool.py:738(__exit__)\n",
      "        9    0.000    0.000  240.683   26.743 /usr/lib/python3.12/multiprocessing/util.py:208(__call__)\n",
      "        1    0.000    0.000  240.683  240.683 /usr/lib/python3.12/multiprocessing/pool.py:654(terminate)\n",
      "        1    0.000    0.000  240.683  240.683 /usr/lib/python3.12/multiprocessing/pool.py:680(_terminate_pool)\n",
      "        1    0.025    0.025  240.639  240.639 /usr/lib/python3.12/multiprocessing/pool.py:671(_help_stuff_finish)\n",
      "        1    0.003    0.003  240.614  240.614 {method 'acquire' of '_multiprocessing.SemLock' objects}\n",
      "      3/1    0.000    0.000  240.611  240.611 /usr/lib/python3.12/threading.py:1018(_bootstrap)\n",
      "       16    0.000    0.000  240.611   15.038 /usr/lib/python3.12/multiprocessing/connection.py:202(send)\n",
      "      3/1    0.000    0.000  240.611  240.611 /usr/lib/python3.12/threading.py:1058(_bootstrap_inner)\n",
      "      3/1    0.000    0.000  240.611  240.611 /usr/lib/python3.12/threading.py:1001(run)\n",
      "        1    0.000    0.000  240.611  240.611 /usr/lib/python3.12/multiprocessing/pool.py:527(_handle_tasks)\n",
      "       21    0.000    0.000  240.610   11.458 /usr/lib/python3.12/multiprocessing/connection.py:406(_send_bytes)\n",
      "       21    0.000    0.000  240.610   11.458 /usr/lib/python3.12/multiprocessing/connection.py:381(_send)\n",
      "       21    0.000    0.000  240.609   11.458 {built-in method posix.write}\n",
      "        1    0.000    0.000  240.609  240.609 /usr/lib/python3.12/multiprocessing/pool.py:573(_handle_results)\n",
      "      351    0.002    0.000  240.465    0.685 /usr/lib/python3.12/multiprocessing/connection.py:1122(wait)\n",
      "      351    0.106    0.000  240.447    0.685 /usr/lib/python3.12/selectors.py:402(select)\n",
      "      176    0.000    0.000  239.043    1.358 /usr/lib/python3.12/multiprocessing/queues.py:374(empty)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Profile the training to see what takes the most time\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "vocab_prof, merges_prof = train_bpe(\n",
    "    input_path=\"/content/data/TinyStoriesV2-GPT4-train.txt\",\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"<|endoftext|>\"]\n",
    ")\n",
    "\n",
    "profiler.disable()\n",
    "\n",
    "s = StringIO()\n",
    "ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "ps.print_stats(20)\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7(a) Tokenizer Experiments — Compression Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyStories vocab/merges already in memory\n",
      "Loaded OpenWebText vocab/merges from disk\n",
      "\n",
      "============================================================\n",
      "Compression Ratio: TinyStories (10K vocab)\n",
      "============================================================\n",
      " Doc     Bytes    Tokens   Bytes/Token\n",
      "----  --------  --------  ------------\n",
      "   1       646       150          4.31\n",
      "   2      1600       393          4.07\n",
      "   3       839       183          4.58\n",
      "   4       714       174          4.10\n",
      "   5       667       159          4.19\n",
      "   6       734       163          4.50\n",
      "   7       911       231          3.94\n",
      "   8      1195       301          3.97\n",
      "   9      1923       452          4.25\n",
      "  10       757       164          4.62\n",
      "\n",
      "  Average compression ratio: 4.25 bytes/token\n",
      "\n",
      "============================================================\n",
      "Compression Ratio: OpenWebText (32K vocab)\n",
      "============================================================\n",
      " Doc     Bytes    Tokens   Bytes/Token\n",
      "----  --------  --------  ------------\n",
      "   1      2292       503          4.56\n",
      "   2      2293       497          4.61\n",
      "   3      4518       931          4.85\n",
      "   4      2985       659          4.53\n",
      "   5      4656       994          4.68\n",
      "   6      2050       437          4.69\n",
      "   7      3463       743          4.66\n",
      "   8      4302      1028          4.18\n",
      "   9      5872      1369          4.29\n",
      "  10      8379      1953          4.29\n",
      "\n",
      "  Average compression ratio: 4.54 bytes/token\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "from ece496b_basics.tokenizer import Tokenizer\n",
    "\n",
    "# --- Load saved vocab/merges ---------------------------------------------------\n",
    "try:\n",
    "    ts_vocab\n",
    "    ts_merges\n",
    "    print(\"TinyStories vocab/merges already in memory\")\n",
    "except NameError:\n",
    "    with open(\"outputs/ts_vocab_10k.pkl\", \"rb\") as f:\n",
    "        ts_vocab = pickle.load(f)\n",
    "    with open(\"outputs/ts_merges_10k.pkl\", \"rb\") as f:\n",
    "        ts_merges = pickle.load(f)\n",
    "    print(\"Loaded TinyStories vocab/merges from disk\")\n",
    "\n",
    "try:\n",
    "    owt_vocab\n",
    "    owt_merges\n",
    "    print(\"OpenWebText vocab/merges already in memory\")\n",
    "except NameError:\n",
    "    with open(\"outputs/owt_vocab_32k.pkl\", \"rb\") as f:\n",
    "        owt_vocab = pickle.load(f)\n",
    "    with open(\"outputs/owt_merges_32k.pkl\", \"rb\") as f:\n",
    "        owt_merges = pickle.load(f)\n",
    "    print(\"Loaded OpenWebText vocab/merges from disk\")\n",
    "\n",
    "# --- Build tokenizers ----------------------------------------------------------\n",
    "ts_tokenizer = Tokenizer(ts_vocab, ts_merges, special_tokens=[\"<|endoftext|>\"])\n",
    "owt_tokenizer = Tokenizer(owt_vocab, owt_merges, special_tokens=[\"<|endoftext|>\"])\n",
    "\n",
    "# --- Sample 10 documents from each training file ------------------------------\n",
    "def sample_documents(path, n=10, seed=42):\n",
    "    \"\"\"Read first ~10 MB, split on <|endoftext|>, and randomly sample n docs.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read(10_000_000)  # ~10 MB is plenty for sampling\n",
    "    docs = [d.strip() for d in text.split(\"<|endoftext|>\") if d.strip()]\n",
    "    random.seed(seed)\n",
    "    return random.sample(docs, min(n, len(docs)))\n",
    "\n",
    "ts_docs = sample_documents(\"/content/data/TinyStoriesV2-GPT4-train.txt\")\n",
    "owt_docs = sample_documents(\"/content/data/owt_train.txt\")\n",
    "\n",
    "# --- Compute compression ratios -----------------------------------------------\n",
    "def compression_ratios(docs, tokenizer, label):\n",
    "    \"\"\"Compute bytes/token for each document and print a summary table.\"\"\"\n",
    "    ratios = []\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Compression Ratio: {label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Doc':>4}  {'Bytes':>8}  {'Tokens':>8}  {'Bytes/Token':>12}\")\n",
    "    print(f\"{'-'*4}  {'-'*8}  {'-'*8}  {'-'*12}\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        n_bytes = len(doc.encode(\"utf-8\"))\n",
    "        token_ids = tokenizer.encode(doc)\n",
    "        n_tokens = len(token_ids)\n",
    "        ratio = n_bytes / n_tokens if n_tokens > 0 else float(\"inf\")\n",
    "        ratios.append(ratio)\n",
    "        print(f\"{i+1:>4}  {n_bytes:>8}  {n_tokens:>8}  {ratio:>12.2f}\")\n",
    "    avg = sum(ratios) / len(ratios)\n",
    "    print(f\"\\n  Average compression ratio: {avg:.2f} bytes/token\")\n",
    "    return ratios\n",
    "\n",
    "ts_ratios = compression_ratios(ts_docs, ts_tokenizer, \"TinyStories (10K vocab)\")\n",
    "owt_ratios = compression_ratios(owt_docs, owt_tokenizer, \"OpenWebText (32K vocab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7(b) Cross-domain Compression — TinyStories tokenizer on OWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Compression Ratio: OWT docs with TinyStories (10K) tokenizer\n",
      "============================================================\n",
      " Doc     Bytes    Tokens   Bytes/Token\n",
      "----  --------  --------  ------------\n",
      "   1      2292       694          3.30\n",
      "   2      2293       665          3.45\n",
      "   3      4518      1884          2.40\n",
      "   4      2985       902          3.31\n",
      "   5      4656      1437          3.24\n",
      "   6      2050       693          2.96\n",
      "   7      3463      1015          3.41\n",
      "   8      4302      1231          3.49\n",
      "   9      5872      1944          3.02\n",
      "  10      8379      2316          3.62\n",
      "\n",
      "  Average compression ratio: 3.22 bytes/token\n"
     ]
    }
   ],
   "source": [
    "# Cross-domain: TinyStories tokenizer on OWT documents\n",
    "owt_cross_ratios = compression_ratios(owt_docs, ts_tokenizer, \"OWT docs with TinyStories (10K) tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 1.01 MB\n",
      "Time: 1.50 s\n",
      "Throughput: 0.67 MB/s\n",
      "\n",
      "Estimated time for The Pile (825 GB): 342.0 hours\n"
     ]
    }
   ],
   "source": [
    "import os                                                                                                           \n",
    "os.chdir(\"/content/LLM-from-scratch/assignment1\") \n",
    "\n",
    "import time, pickle                                                                                                 \n",
    "from ece496b_basics.tokenizer import Tokenizer                                                                      \n",
    "                                                                                                                      \n",
    "  # Load OWT tokenizer                                                                                              \n",
    "with open(\"outputs/owt_vocab_32k.pkl\", \"rb\") as f:\n",
    "    owt_vocab = pickle.load(f)\n",
    "with open(\"outputs/owt_merges_32k.pkl\", \"rb\") as f:\n",
    "    owt_merges = pickle.load(f)\n",
    "tokenizer = Tokenizer(owt_vocab, owt_merges, special_tokens=[\"<|endoftext|>\"])\n",
    "\n",
    "  # Read a chunk of OWT text\n",
    "with open(\"/content/data/owt_train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sample = f.read(1_000_000)  # ~1 MB\n",
    "\n",
    "sample_bytes = len(sample.encode(\"utf-8\"))\n",
    "\n",
    "start = time.time()\n",
    "tokens = tokenizer.encode(sample)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "throughput = sample_bytes / elapsed\n",
    "print(f\"Sample size: {sample_bytes / 1e6:.2f} MB\")\n",
    "print(f\"Time: {elapsed:.2f} s\")\n",
    "print(f\"Throughput: {throughput / 1e6:.2f} MB/s\")\n",
    "\n",
    "pile_size_gb = 825\n",
    "pile_time_s = (pile_size_gb * 1e9) / throughput\n",
    "print(f\"\\nEstimated time for The Pile (825 GB): {pile_time_s / 3600:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.7 (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from ece496b_basics.tokenizer import Tokenizer\n",
    "\n",
    "NUM_WORKERS = cpu_count()\n",
    "print(f\"Using {NUM_WORKERS} workers\")\n",
    "\n",
    "# --- Load tokenizers -----------------------------------------------------------\n",
    "with open(\"outputs/ts_vocab_10k.pkl\", \"rb\") as f:\n",
    "    ts_vocab = pickle.load(f)\n",
    "with open(\"outputs/ts_merges_10k.pkl\", \"rb\") as f:\n",
    "    ts_merges = pickle.load(f)\n",
    "with open(\"outputs/owt_vocab_32k.pkl\", \"rb\") as f:\n",
    "    owt_vocab = pickle.load(f)\n",
    "with open(\"outputs/owt_merges_32k.pkl\", \"rb\") as f:\n",
    "    owt_merges = pickle.load(f)\n",
    "\n",
    "# --- Multiprocessing helpers ---------------------------------------------------\n",
    "# Each worker gets its own Tokenizer instance via the initializer,\n",
    "# avoiding pickling the tokenizer on every chunk.\n",
    "_worker_tokenizer = None\n",
    "\n",
    "def init_worker(vocab, merges):\n",
    "    \"\"\"Initialize a per-worker tokenizer (called once per process).\"\"\"\n",
    "    global _worker_tokenizer\n",
    "    _worker_tokenizer = Tokenizer(vocab, merges, special_tokens=[\"<|endoftext|>\"])\n",
    "\n",
    "def encode_chunk(chunk):\n",
    "    \"\"\"Encode a text chunk using the worker's tokenizer.\"\"\"\n",
    "    return _worker_tokenizer.encode(chunk)\n",
    "\n",
    "def parallel_encode(text, vocab, merges, num_workers):\n",
    "    \"\"\"Split text on <|endoftext|> boundaries and encode chunks in parallel.\n",
    "\n",
    "    Splitting on document boundaries ensures we never break a token mid-word.\n",
    "    \"\"\"\n",
    "    docs = text.split(\"<|endoftext|>\")\n",
    "    # Distribute documents into roughly equal groups\n",
    "    chunk_size = max(1, len(docs) // num_workers)\n",
    "    chunks = []\n",
    "    for i in range(0, len(docs), chunk_size):\n",
    "        chunks.append(\"<|endoftext|>\".join(docs[i:i + chunk_size]))\n",
    "\n",
    "    print(f\"  Split into {len(chunks)} chunks across {num_workers} workers...\", flush=True)\n",
    "\n",
    "    with Pool(num_workers, initializer=init_worker, initargs=(vocab, merges)) as pool:\n",
    "        results = pool.map(encode_chunk, chunks)\n",
    "\n",
    "    # Flatten list of lists into a single token ID sequence\n",
    "    all_ids = []\n",
    "    for r in results:\n",
    "        all_ids.extend(r)\n",
    "    return all_ids\n",
    "\n",
    "# --- Encode all datasets -------------------------------------------------------\n",
    "# Each (name, vocab, merges, input_path, output_path) tuple defines one encoding job.\n",
    "# Token IDs are stored as uint16: holds 0–65,535, enough for both 10K and 32K vocabs,\n",
    "# and uses 2 bytes/token (4x smaller than default int64).\n",
    "datasets = [\n",
    "    (\"TinyStories train\", ts_vocab, ts_merges, \"/content/data/TinyStoriesV2-GPT4-train.txt\", \"outputs/ts_train.npy\"),\n",
    "    (\"TinyStories valid\", ts_vocab, ts_merges, \"/content/data/TinyStoriesV2-GPT4-valid.txt\", \"outputs/ts_valid.npy\"),\n",
    "    (\"OWT train\", owt_vocab, owt_merges, \"/content/data/owt_train.txt\", \"outputs/owt_train.npy\"),\n",
    "    (\"OWT valid\", owt_vocab, owt_merges, \"/content/data/owt_valid.txt\", \"outputs/owt_valid.npy\"),\n",
    "]\n",
    "\n",
    "for name, vocab, merges, path, out_path in datasets:\n",
    "    print(f\"Encoding {name}...\", flush=True)\n",
    "    start = time.time()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    ids = parallel_encode(text, vocab, merges, NUM_WORKERS)\n",
    "    arr = np.array(ids, dtype=np.uint16)\n",
    "    np.save(out_path, arr)\n",
    "    elapsed = time.time() - start\n",
    "    mb_per_sec = len(text.encode(\"utf-8\")) / elapsed / 1e6\n",
    "    print(f\"  {len(arr):,} tokens -> {out_path} ({arr.nbytes / 1e6:.1f} MB) in {elapsed:.1f}s ({mb_per_sec:.1f} MB/s)\", flush=True)\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Problem (learning_rate_tuning): Tuning the learning rate (1 pt)\n",
    "\n",
    "Run the SGD toy example with three learning rates: 1e1, 1e2, and 1e3, for just 10 training iterations. Observe whether loss decays faster, slower, or diverges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self, params, lr=1e-3):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                t = state.get(\"t\", 0)\n",
    "                grad = p.grad.data\n",
    "                p.data -= lr / math.sqrt(t + 1) * grad\n",
    "                state[\"t\"] = t + 1\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Run the toy example for lr = 1e1, 1e2, 1e3 (10 steps each)\n",
    "learning_rates = [1e1, 1e2, 1e3]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    torch.manual_seed(42)\n",
    "    weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "    opt = SGD([weights], lr=lr)\n",
    "    losses = []\n",
    "    for t in range(10):\n",
    "        opt.zero_grad()\n",
    "        loss = (weights**2).mean()\n",
    "        losses.append(loss.cpu().item())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    results[lr] = losses\n",
    "\n",
    "# Print results\n",
    "print(f\"{'Step':>4}\", end=\"\")\n",
    "for lr in learning_rates:\n",
    "    print(f\"  {'lr='+str(lr):>12}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 44)\n",
    "\n",
    "for t in range(10):\n",
    "    print(f\"{t:>4}\", end=\"\")\n",
    "    for lr in learning_rates:\n",
    "        print(f\"  {results[lr][t]:>12.4f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for lr in learning_rates:\n",
    "    first, last = results[lr][0], results[lr][-1]\n",
    "    if last < first * 0.5:\n",
    "        behavior = \"decays well\"\n",
    "    elif last < first:\n",
    "        behavior = \"decays slowly\"\n",
    "    else:\n",
    "        behavior = \"DIVERGES\"\n",
    "    print(f\"  lr={lr:.0e}: {first:.4f} -> {last:.4f}  ({behavior})\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
