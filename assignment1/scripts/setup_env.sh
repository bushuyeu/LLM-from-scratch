#!/usr/bin/env bash
set -euo pipefail
set -x

log() {
  printf '[setup_env] %s\n' "$*" >&2
}

# Root of the project snapshot (contains repo/ and run_metadata/)
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"

# Load variables from .env if present so downstream commands inherit them
ENV_FILE="${PROJECT_ROOT}/.env"
if [[ -f "${ENV_FILE}" ]]; then
  # shellcheck disable=SC1090  # dynamic path from repository root
  set -a
  source "${ENV_FILE}"
  set +a
  log "Loaded environment variables from ${ENV_FILE}"
fi

# Shared uv-managed environment (overridable via KOA_SHARED_ENV); lives outside job snapshots
SHARED_ENV_DIR="${KOA_SHARED_ENV:-${PROJECT_ROOT}/../.venv}"
mkdir -p "${SHARED_ENV_DIR}"
ENV_PYTHON="${SHARED_ENV_DIR}/bin/python"

# Add uv to PATH
export PATH="${HOME}/.local/bin:${PATH}"

# Cache location for environment hashes (used to detect changes between runs)
ENV_CACHE_DIR="${SHARED_ENV_DIR}/.koa"
ENV_HASH_CACHE="${ENV_CACHE_DIR}/env_hashes.json"

# Locate the manifest generated by `koa submit` (contains env_hashes.json)
RUN_METADATA_DIR="${KOA_RUN_METADATA_DIR:-}"
if [[ -z "${RUN_METADATA_DIR}" && -n "${KOA_RUN_DIR:-}" ]]; then
  RUN_METADATA_DIR="${KOA_RUN_DIR}/run_metadata"
fi
if [[ -z "${RUN_METADATA_DIR}" && -n "${KOA_ML_RESULTS_ROOT:-}" && -n "${SLURM_JOB_ID:-}" ]]; then
  RUN_METADATA_DIR="${KOA_ML_RESULTS_ROOT}/${SLURM_JOB_ID}/run_metadata"
fi

KOA_PROJECT_ROOT="${KOA_PROJECT_ROOT:-${PROJECT_ROOT}}"

# Install local CUDA Toolkit (configurable via CUDA_MINOR_VERSION).
CUDA_MINOR_VERSION="${CUDA_MINOR_VERSION:-12.8}"

case "${CUDA_MINOR_VERSION}" in
  12.4)
    CUDA_FULL_VERSION="${CUDA_FULL_VERSION:-12.4.1}"
    CUDA_INSTALLER_TAG="${CUDA_INSTALLER_TAG:-550.54.15}"
    ;;
  12.8)
    CUDA_FULL_VERSION="${CUDA_FULL_VERSION:-12.8.0}"
    CUDA_INSTALLER_TAG="${CUDA_INSTALLER_TAG:-570.86.10}"
    ;;
  *)
    if [[ -z "${CUDA_FULL_VERSION:-}" || -z "${CUDA_INSTALLER_TAG:-}" ]]; then
      log "Unsupported CUDA_MINOR_VERSION='${CUDA_MINOR_VERSION}'. Set CUDA_FULL_VERSION and CUDA_INSTALLER_TAG to override."
      exit 1
    fi
    ;;
esac

CUDA_INSTALL_ROOT="${KOA_PROJECT_ROOT}/cuda-${CUDA_MINOR_VERSION}"
CUDA_INSTALLER_FILE="${CUDA_INSTALLER_FILE:-cuda_${CUDA_FULL_VERSION}_${CUDA_INSTALLER_TAG}_linux.run}"
CUDA_INSTALLER_PATH="${KOA_PROJECT_ROOT}/${CUDA_INSTALLER_FILE}"
CUDA_DOWNLOAD_URL="https://developer.download.nvidia.com/compute/cuda/${CUDA_FULL_VERSION}/local_installers/${CUDA_INSTALLER_FILE}"

if [[ ! -x "${CUDA_INSTALL_ROOT}/bin/nvcc" ]]; then
  log "Ensuring CUDA Toolkit ${CUDA_MINOR_VERSION} exists at ${CUDA_INSTALL_ROOT}"
  mkdir -p "${CUDA_INSTALL_ROOT}"

  if [[ ! -f "${CUDA_INSTALLER_PATH}" ]]; then
    log "Downloading CUDA Toolkit installer from ${CUDA_DOWNLOAD_URL}"
    curl -fL "${CUDA_DOWNLOAD_URL}" -o "${CUDA_INSTALLER_PATH}"
  fi

  chmod +x "${CUDA_INSTALLER_PATH}"
  "${CUDA_INSTALLER_PATH}" --silent --toolkit --toolkitpath="${CUDA_INSTALL_ROOT}"
  rm -f "${CUDA_INSTALLER_PATH}"
else
  log "Found existing CUDA Toolkit at ${CUDA_INSTALL_ROOT}; skipping install"
fi

export CUDA_HOME="${CUDA_INSTALL_ROOT}"
export CUDA_PATH="${CUDA_INSTALL_ROOT}"
export PATH="${CUDA_INSTALL_ROOT}/bin:${PATH}"
if [[ -n "${LD_LIBRARY_PATH:-}" ]]; then
  export LD_LIBRARY_PATH="${CUDA_INSTALL_ROOT}/lib64:${LD_LIBRARY_PATH}"
else
  export LD_LIBRARY_PATH="${CUDA_INSTALL_ROOT}/lib64"
fi

# Prefer python3, fall back to python
python_bin="$(command -v python3 || command -v python)"

if [[ -z "${python_bin}" ]]; then
  log "No python interpreter found" >&2
  exit 1
fi

# Determine whether we need to rebuild or refresh the shared environment
recreate=0
if [[ ! -x "${ENV_PYTHON}" ]]; then
  recreate=1
fi

ENV_HASH_SOURCE=""
if [[ -n "${RUN_METADATA_DIR}" && -f "${RUN_METADATA_DIR}/env_hashes.json" ]]; then
  ENV_HASH_SOURCE="${RUN_METADATA_DIR}/env_hashes.json"
fi

# Compare the last synced env hashes with the current ones; rebuild if they differ
if [[ "${recreate}" -eq 0 && -n "${ENV_HASH_SOURCE}" ]]; then
  mkdir -p "${ENV_CACHE_DIR}"
  if [[ ! -f "${ENV_HASH_CACHE}" ]] || ! cmp -s "${ENV_HASH_SOURCE}" "${ENV_HASH_CACHE}"; then
    recreate=1
  fi
fi

export UV_PROJECT_ENVIRONMENT="${SHARED_ENV_DIR}"
if [[ "${recreate}" -eq 1 ]]; then
  log "Recreating shared environment at ${SHARED_ENV_DIR}"
  # Ensure uv is available for managing the shared environment
  if ! uv --help >/dev/null 2>&1; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
  fi

  # (Re)create the uv-managed environment and install dependencies from this repo snapshot
  uv venv --clear "${SHARED_ENV_DIR}"
  uv sync --extra hpc

  if [[ -n "${ENV_HASH_SOURCE}" ]]; then
    mkdir -p "${ENV_CACHE_DIR}"
    cp "${ENV_HASH_SOURCE}" "${ENV_HASH_CACHE}"
  fi
else
  log "Shared environment already up to date; skipping rebuild"
fi
