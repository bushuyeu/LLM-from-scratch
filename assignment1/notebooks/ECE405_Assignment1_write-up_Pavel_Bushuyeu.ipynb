{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The Unicode Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What Unicode character does chr(0) return?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = chr(0)\n",
    "print(c)          # nothing visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chr(0) returns the Unicode character null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) How does this character's string representation ```(__repr__())``` differ from its printed representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it shows an escaped form (```'\\x00'```), which makes the character visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(c) What happens when this character occurs in text? It may be helpful to play around with the\n",
    "following in your Python interpreter and see if it matches your expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(0)\n",
    "print(chr(0))\n",
    "s = \"this is a test\" + chr(0) + \"string\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'this is a teststring' gets printed. However, there is still a null between test and string as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Unicode Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the reasons is space efficiency for English/ ASCII characters (e.g.: character \"A\" in UTF-8 is 1 byte, in UTF-16 - 2 bytes, in UTF-32 - 4 bytes). Since most large-scale corpora are predominantly ASCII, UTF-8 yields a much smaller byte sequence to learn patterns over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "  return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is incorrect as it decodes each byte independently. That works for ASCII, however, as UTF-8 encodes some characters using more than 1 byte, hence it might not always work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"„Åì\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"üåç\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. Make sure to add the TinyStories <|endoftext|> special token to the vocabulary. Serialize the resulting vocabulary and merges to disk for further inspection.\n",
    "\n",
    "How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?\n",
    "\n",
    "Resource requirements: ‚â§30 minutes (no GPUs), ‚â§30GB RAM Hint You should be able to get under 2 minutes for BPE training using multiprocessing during\n",
    "pretokenization and the following two facts:\n",
    "\n",
    "* (a) The <|endoftext|> token delimits documents in the data files.\n",
    "* (b) The <|endoftext|> token is handled as a special case before the BPE merges are applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training time: 1029.17 seconds (17.15 minutes)\n",
    "* Peak memory: 0.08 GB\n",
    "\n",
    "Top longest tokens:\n",
    "* ID 7159: ' accomplishment' (15 bytes)\n",
    "* ID 9142: ' disappointment' (15 bytes)\n",
    "* ID 9378: ' responsibility' (15 bytes)\n",
    "\n",
    "Does it make sense? Yes. frequent word become a single token with a leading space per used regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Profile your code. What part of the tokenizer training process takes the most time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "5725983 function calls (5725798 primitive calls) in 363.504 seconds\n",
    "\n",
    "Ordered by: cumulative time\n",
    "List reduced from 360 to 20 due to restriction <20>\n",
    "\n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "      362  359.042    0.992  700.703    1.936 {built-in method time.sleep}\n",
    "      3/2    0.000    0.000  363.536  181.768 /usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3512(run_code)\n",
    "      3/2    0.000    0.000  363.536  181.768 {built-in method builtins.exec}\n",
    "      109    0.000    0.000  341.941    3.137 /usr/lib/python3.12/multiprocessing/pool.py:500(_wait_for_updates)\n",
    "      221    0.002    0.000  341.940    1.547 /usr/lib/python3.12/multiprocessing/connection.py:1122(wait)\n",
    "      221    0.793    0.004  341.836    1.547 /usr/lib/python3.12/selectors.py:402(select)\n",
    "        1    0.000    0.000  340.641  340.641 /usr/lib/python3.12/multiprocessing/pool.py:738(__exit__)\n",
    "        9    0.000    0.000  340.592   37.844 /usr/lib/python3.12/multiprocessing/util.py:208(__call__)\n",
    "        1    0.000    0.000  340.592  340.592 /usr/lib/python3.12/multiprocessing/pool.py:654(terminate)\n",
    "        1    0.000    0.000  340.592  340.592 /usr/lib/python3.12/multiprocessing/pool.py:680(_terminate_pool)\n",
    "        1    0.004    0.004  340.542  340.542 /usr/lib/python3.12/multiprocessing/pool.py:671(_help_stuff_finish)\n",
    "        1    0.004    0.004  340.539  340.539 {method 'acquire' of '_multiprocessing.SemLock' objects}\n",
    "      3/1    0.000    0.000  340.535  340.535 /usr/lib/python3.12/threading.py:1018(_bootstrap)\n",
    "       16    0.000    0.000  340.535   21.283 /usr/lib/python3.12/multiprocessing/connection.py:202(send)\n",
    "      3/1    0.000    0.000  340.535  340.535 /usr/lib/python3.12/threading.py:1058(_bootstrap_inner)\n",
    "      3/1    0.000    0.000  340.535  340.535 /usr/lib/python3.12/threading.py:1001(run)\n",
    "        1    0.000    0.000  340.534  340.534 /usr/lib/python3.12/multiprocessing/pool.py:527(_handle_tasks)\n",
    "       21    0.000    0.000  340.534   16.216 /usr/lib/python3.12/multiprocessing/connection.py:406(_send_bytes)\n",
    "       21    0.000    0.000  340.534   16.216 /usr/lib/python3.12/multiprocessing/connection.py:381(_send)\n",
    "       21    0.000    0.000  340.534   16.216 {built-in method posix.write}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slowest part is the parallel pretokenization pipeline and process synchronization (main process waiting for workers), rather than the core BPE training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points)\n",
    "(a) Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection.\n",
    "\n",
    "* What is the longest token in the vocabulary?\n",
    "* Does it make sense?\n",
    "\n",
    "Resource requirements: ‚â§12 hours (no GPUs), ‚â§100GB RAM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "vocab_path = os.path.join(os.path.dirname(__file__) if \"__file__\" in dir() else \".\", \"..\", \"outputs\", \"owt_vocab_32k.pkl\")\n",
    "# Adjust path as needed for your environment\n",
    "with open(vocab_path, \"rb\") as f:\n",
    "    owt_vocab = pickle.load(f)\n",
    "\n",
    "longest_id, longest_token = max(owt_vocab.items(), key=lambda x: len(x[1]))\n",
    "try:\n",
    "    decoded = longest_token.decode(\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    decoded = repr(longest_token)\n",
    "\n",
    "print(f\"Longest token: {decoded!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest token is a string ('√É√Ç√É√Ç...'); \n",
    "Should make sense as encoding artifacts are common in OpenWebText and BPE aggressively merges frequent repeated patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TinyStories tokenizer learns simple vocabulary (' accomplishment', ' disappointment') while the OWT tokenizer reflects web text including encoding artifacts, separator lines, URL fragments (http, www), and political and technical terms (' unconstitutional', ' cryptocurrencies')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7(a) Tokenizer Experiments ‚Äî Compression Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A) Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyS-\n",
    "tories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these\n",
    "sampled documents into integer IDs. What is each tokenizer's compression ratio (bytes/token)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average compression ratios (bytes/token) over 10 sampled documents:\n",
    "TinyStories (10K vocab): 4.25 bytes/token\n",
    "OpenWebText (32K vocab): 4.54 bytes/token\n",
    "\n",
    "The difference is due to vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B) What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Com-\n",
    "pare the compression ratio and/or qualitatively describe what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing OWT text with the TinyStories tokenizer yields a compression ratio of 3.22 bytes/token (vs 4.54\n",
    "with the native OWT tokenizer). The TinyStories vocabulary doesn't merged tokens for technical, political, and web-specific terms common in OWT, so the tokenizer falls back to shorter or single-byte tokens, producing more tokens for the same text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) Throughput: ~0.67 MB/s (measured on a 1 MB OWT sample). At that rate, tokenizing The Pile (825 GB) would take approximately 342 hours (~14 days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(D) Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We'll use this later to train our language\n",
    "model. We recommend serializing the token IDs as a NumPy array of datatype uint16. Why is\n",
    "uint16 an appropriate choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why `uint16`? It stores values 0‚Äì65,535, which covers both vocab sizes and is the smallest type that fits the vocab sizes, minimizing storage and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4.2 Problem (learning_rate_tuning): Tuning the learning rate (1 point)\n\nRun the SGD toy example (Eq. 20: Œ∏_{t+1} = Œ∏_t ‚àí Œ±/‚àö(t+1) ¬∑ ‚àáL) with lr ‚àà {1e1, 1e2, 1e3} for 10 steps.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport math\nfrom typing import Optional\nfrom collections.abc import Callable\n\n\nclass SGD(torch.optim.Optimizer):\n    \"\"\"SGD with decaying learning rate: Œ∏_{t+1} = Œ∏_t - (Œ± / ‚àö(t+1)) ¬∑ ‚àáL(Œ∏_t; B_t)\"\"\"\n\n    def __init__(self, params, lr=1e-3):\n        if lr < 0:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        defaults = {\"lr\": lr}\n        super().__init__(params, defaults)\n\n    def step(self, closure: Optional[Callable] = None):\n        loss = None if closure is None else closure()\n        for group in self.param_groups:\n            lr = group[\"lr\"]\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                state = self.state[p]\n                t = state.get(\"t\", 0)\n                grad = p.grad.data\n                p.data -= lr / math.sqrt(t + 1) * grad\n                state[\"t\"] = t + 1\n        return loss\n\n\nlearning_rates = [1e1, 1e2, 1e3]\nresults = {}\n\nfor lr in learning_rates:\n    torch.manual_seed(42)\n    weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n    opt = SGD([weights], lr=lr)\n    losses = []\n    for t in range(10):\n        opt.zero_grad()\n        loss = (weights**2).mean()\n        losses.append(loss.cpu().item())\n        loss.backward()\n        opt.step()\n    results[lr] = losses\n\nprint(f\"{'Step':>4}\", end=\"\")\nfor lr in learning_rates:\n    print(f\"  {'lr='+str(lr):>12}\", end=\"\")\nprint()\nprint(\"-\" * 44)\nfor t in range(10):\n    print(f\"{t:>4}\", end=\"\")\n    for lr in learning_rates:\n        print(f\"  {results[lr][t]:>12.4f}\", end=\"\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "- **lr=10**: Loss decays steadily but slowly (24.2 ‚Üí 3.2 after 10 steps) ‚Äî conservative step size, stable but gradual progress.\n- **lr=100**: Loss converges rapidly to near zero by step 4 ‚Äî the sweet spot where the optimizer takes big enough steps to converge quickly without overshooting.\n- **lr=1000**: Loss explodes immediately (24.2 ‚Üí 8,725 after 1 step, then to 10¬π‚Å∏) ‚Äî the learning rate massively overshoots the minimum, causing divergence.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4.1.1 Problem (adamw): Implement AdamW (2 points)\n\nImplemented in `ece496b_basics/optimizer.py`. Subclasses `torch.optim.Optimizer`.\n\n`step` implements Algorithm 1:\n1. `m ‚Üê Œ≤‚ÇÅ¬∑m + (1‚àíŒ≤‚ÇÅ)¬∑g` ‚Äî first moment update\n2. `v ‚Üê Œ≤‚ÇÇ¬∑v + (1‚àíŒ≤‚ÇÇ)¬∑g¬≤` ‚Äî second moment update\n3. Bias-correct: `mÃÇ = m/(1‚àíŒ≤‚ÇÅ·µó)`, `vÃÇ = v/(1‚àíŒ≤‚ÇÇ·µó)`\n4. `Œ∏ ‚Üê Œ∏ ‚àí Œ± ¬∑ mÃÇ/(‚àövÃÇ + Œµ)` ‚Äî parameter update\n5. `Œ∏ ‚Üê Œ∏ ‚àí Œ±¬∑Œª¬∑Œ∏` ‚Äî decoupled weight decay",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport math\nfrom torch.optim import Optimizer\n\n\nclass AdamW(Optimizer):\n    \"\"\"AdamW optimizer ‚Äî Adam with decoupled weight decay.\"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            lr = group[\"lr\"]\n            beta1, beta2 = group[\"betas\"]\n            eps = group[\"eps\"]\n            weight_decay = group[\"weight_decay\"]\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                state = self.state[p]\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"m\"] = torch.zeros_like(p)\n                    state[\"v\"] = torch.zeros_like(p)\n\n                state[\"step\"] += 1\n                t = state[\"step\"]\n                m, v = state[\"m\"], state[\"v\"]\n\n                # Update biased moment estimates\n                m.mul_(beta1).add_(grad, alpha=1 - beta1)\n                v.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                # Bias correction\n                m_hat = m / (1 - beta1 ** t)\n                v_hat = v / (1 - beta2 ** t)\n\n                # Adam update\n                p.add_(m_hat / (v_hat.sqrt() + eps), alpha=-lr)\n\n                # Decoupled weight decay\n                if weight_decay != 0:\n                    p.add_(p, alpha=-lr * weight_decay)\n\n        return loss\n\n\n# Quick sanity check\ntorch.manual_seed(0)\nmodel = torch.nn.Linear(3, 2)\nopt = AdamW(model.parameters(), lr=1e-2, weight_decay=0.01)\nfor _ in range(5):\n    opt.zero_grad()\n    loss = (model(torch.randn(4, 3)) ** 2).mean()\n    loss.backward()\n    opt.step()\nprint(f\"AdamW sanity check ‚Äî final loss: {loss.item():.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.3 Problem (adamwAccounting): Resource accounting with AdamW (2 points)\n\n**(a)** Peak memory expressions (float32 = 4 bytes per value):\n\nLet B = batch_size, S = context_length, D = d_model, H = num_heads, L = num_layers, V = vocab_size, F = d_ff = 4D.\n\n**Parameters (P):**\nPer block: 4D¬≤ (QKV+O) + 2¬∑D¬∑F (FFN W1,W2) + 2D (two RMSNorms) = 12D¬≤ + 2D\nFull model: **P = L¬∑(12D¬≤ + 2D) + 2VD + D**\nMemory: 4P bytes\n\n**Gradients:** 4P bytes\n**Optimizer state (AdamW):** m + v = 8P bytes\n\n**Activations** (stored for backward, per block √óL):\n- RMSNorm inputs (√ó2): 2BSD\n- Q, K, V projections: 3BSD\n- Softmax output: BHS¬≤\n- Attention output + residuals + FFN intermediates: 11BSD\n\nPer-block total: **16BSD + BHS¬≤**\nModel-level: 2BSD + 2BSV\n\n**Total peak memory = 16P + 4¬∑BS¬∑[L¬∑(16D + HS) + 2D + 2V] bytes**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**(b)** Max batch size for GPT-2 XL on 80 GB A100:\n\nGPT-2 XL: L=48, D=1600, H=25, S=1024, V=50257, F=6400.\n\nP = 48¬∑(12¬∑1600¬≤ + 2¬∑1600) + 2¬∑50257¬∑1600 + 1600 = **1.64B parameters**\nFixed cost = 16P = **26.17 GB**\n\nActivation cost per sample ‚âà **10.49 GB**\nMemory (GB) ‚âà 10.49¬∑B + 26.17\n\n80 ‚â• 10.49¬∑B + 26.17 ‚Üí B ‚â§ 5.13 ‚Üí **B_max = 5**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**(c)** AdamW FLOPs per step:\n\nPer parameter: ~15 FLOPs (3 for m update, 4 for v update, 2 for bias corrections, 4 for Adam update, 2 for weight decay).\n\nFor GPT-2 XL: 15 √ó 1.64B ‚âà **24.5 GFLOPs** per optimizer step",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**(d)** GPT-2 XL training time on a single A100 (B=1024, 400K steps, 50% MFU):\n\nForward FLOPs per step (B=1024): **3,591 TFLOPs**\nBackward ‚âà 2√ó forward: 7,182 TFLOPs\nTotal per step: **10,773 TFLOPs**\n\nA100 at 50% MFU: 19.5 √ó 0.5 = 9.75 TFLOP/s\nTime per step: 10,773 / 9.75 = 1,105 seconds\nTotal: 1,105 √ó 400,000 = **5,116 days ‚âà 14.0 years**\n\nCompletely impractical on a single GPU. Real training uses hundreds of GPUs with data/tensor parallelism.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}